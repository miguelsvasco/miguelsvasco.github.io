<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Miguel Vasco </title> <meta name="author" content="Miguel Vasco"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design "> <meta name="keywords" content="machine-learning, reinforcement-learning, rl"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8F%8E%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://miguelsvasco.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/files/CV.pdf">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Miguel</span> Vasco </h1> <p class="desc">miguelsv [at] kth.se</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic_2025.jpg" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic_2025.jpg?5832b3d96a8ea14fa7cceb4c01ab29e4" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic_2025.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>Hi, I’m Miguel Vasco, a Postdoctoral Researcher at <a href="https://www.kth.se/en" rel="external nofollow noopener" target="_blank">KTH Royal Institute of Technology</a> in Stockholm, advised by <a href="https://www.csc.kth.se/~danik/" rel="external nofollow noopener" target="_blank">Danica Kragic</a>.</p> <p>My long-term goal is to build artificial agents that can <strong>co-exist</strong> with human in real and virtual environments. My research explores how reinforcement learning can be used to develop multimodal agents that act <a href="https://arxiv.org/abs/2406.12563v1" rel="external nofollow noopener" target="_blank">effectively</a>, while remaining <a href="https://arxiv.org/abs/2202.03390" rel="external nofollow noopener" target="_blank">robust</a> to changes in the environment and <a href="https://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1942924&amp;dswid=1366" rel="external nofollow noopener" target="_blank">adaptable</a> to human preferences. I also focus on the <a href="https://openreview.net/forum?id=3f8i9GlBzu" rel="external nofollow noopener" target="_blank">alignment</a> between human and machine perception, studying how to learn multimodal representations <a href="https://arxiv.org/abs/2502.03081/" rel="external nofollow noopener" target="_blank">grounded</a> in human sensory processing.</p> <p>I earned my Ph.D. from <a href="https://tecnico.ulisboa.pt/en/" rel="external nofollow noopener" target="_blank">Instituto Superior Tecnico</a>, where my work on multimodal reinforcement learning was honored with the <a href="https://www.appia.pt/2024/10/07/vencedor-do-concurso-melhor-tese-de-doutoramento/" rel="external nofollow noopener" target="_blank">Best PhD Thesis in AI in Portugal</a>. I also interned at <a href="https://ai.sony/" rel="external nofollow noopener" target="_blank">Sony AI</a>, where I developed superhuman multimodal RL agents.</p> <p>Feel free to explore my work here or connect with me to discuss potential collaborations — I am currently on the job market for industry positions.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Mar 10, 2025</th> <td> New preprints on the alignment of image models for visual decoding from the brain <a href="https://arxiv.org/abs/2502.03081/" rel="external nofollow noopener" target="_blank">(link)</a> and on a framework for the long-term co-existence between humans and artificial agents <a href="https://arxiv.org/abs/2502.04809/" rel="external nofollow noopener" target="_blank">(link)</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 28, 2025</th> <td> Our paper on exploring early stopping bias in the low-data regime of deep learning has been accepted at ICASSP <a href="https://arxiv.org/abs/2406.12563v1" rel="external nofollow noopener" target="_blank">(link)</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 27, 2025</th> <td> Our work on sample-efficient adaptation of reward models for preference-based RL has been accepted at ICRA <a href="https://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1942924&amp;dswid=1366" rel="external nofollow noopener" target="_blank">(link)</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 16, 2024</th> <td> Our work on reducing variance in meta-learning was accepted at TMLR <a href="https://arxiv.org/abs/2410.01476" rel="external nofollow noopener" target="_blank">(link)</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 26, 2024</th> <td> Two Papers Accepted at NeurIPS 2024! Our first work explores foundation models of chemical data for human olfaction (<strong>Spotlight</strong>, <a href="https://arxiv.org/abs/2410.01476" rel="external nofollow noopener" target="_blank">(link)</a>). Our second work explores learning decision-making algorithms that scale to arbitrarily large observation spaces (<strong>Poster</strong>, <a href="https://arxiv.org/abs/2402.15393" rel="external nofollow noopener" target="_blank">(link)</a>). </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 04, 2024</th> <td> Won the <a href="https://www.appia.pt/2024/10/07/vencedor-do-concurso-melhor-tese-de-doutoramento/" rel="external nofollow noopener" target="_blank">Best PhD Thesis in AI in Portugal</a> award by the Portuguese Association for Artificial Intelligence (APPIA). </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 10, 2024</th> <td> Our paper on <a href="https://arxiv.org/abs/2406.12563v1" rel="external nofollow noopener" target="_blank">super-human autonomous racing</a> won an <strong>Outstanding Paper Award at RLC 2024</strong>! </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#9f0036"> <a href="https://neurips.cc" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/olfaction_neurips.png" sizes="200px"> <img src="/assets/img/publication_preview/olfaction_neurips.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="olfaction_neurips.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="taleb2024can" class="col-sm-8"> <div class="title">Can Transformers Smell Like Humans?</div> <div class="author"> Farzaneh Taleb, <em>Miguel Vasco</em>, Antonio H. Ribeiro, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Mårten Björkman, Danica Kragic' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In The Thirty-eighth Annual Conference on Neural Information Processing Systems</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Spotlight</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=3f8i9GlBzu" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Spotlight Presentation at NeurIPS 2024</p> </div> <div class="abstract hidden"> <p>The human brain encodes stimuli from the environment into representations that form a sensory perception of the world. Despite recent advances in understanding visual and auditory perception, olfaction remains an under-explored topic in the machine learning community due to the lack of large-scale datasets annotated with labels related to human olfactory perception. In this work, we ask the question of whether transformer models trained on chemical structures encode representations that are aligned with human olfactory perception, i.e., \emphcan transformers smell like humans? We demonstrate, by means of three analyses, that representations encoded from transformers pre-trained on general chemical structures are highly aligned with human olfactory perception. We use 5 different datasets and 3 different types of perceptual representations to show that the representations encoded by transformer models are able to predict 1) labels associated with odorants‌‌ provided by experts; 2) ratings provided by human participants with respect to pre-defined descriptors; 3) similarity ratings between odorants provided by human participants. Finally, we also evaluate the extent to which this alignment is associated with chemical features of odorants known to be relevant for olfactory perception.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#9f0036"> <a href="https://neurips.cc" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/neuralsolver.gif" sizes="200px"> <img src="/assets/img/publication_preview/neuralsolver.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="neuralsolver.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="esteves2024neuralsolver" class="col-sm-8"> <div class="title">NeuralSolver: Learning Algorithms For Consistent and Efficient Extrapolation Across General Tasks</div> <div class="author"> Bernardo Esteves, <em>Miguel Vasco</em>, and Francisco S. Melo </div> <div class="periodical"> <em>In The Thirty-eighth Annual Conference on Neural Information Processing Systems</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=IxRf7Q3s5e" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>We contribute NeuralSolver, a novel recurrent solver that can efficiently and consistently extrapolate, i.e., learn algorithms from smaller problems (in terms of observation size) and execute those algorithms in large problems. Contrary to previous recurrent solvers, NeuralSolver can be naturally applied in both same-size problems, where the input and output sizes are the same, and in different-size problems, where the size of the input and output differ. To allow for this versatility, we design NeuralSolver with three main components: a recurrent module, that iteratively processes input information at different scales, a processing module, responsible for aggregating the previously processed information, and a curriculum-based training scheme, that improves the extrapolation performance of the method. To evaluate our method we introduce a set of novel different-size tasks and we show that NeuralSolver consistently outperforms the prior state-of-the-art recurrent solvers in extrapolating to larger problems, considering smaller training problems and requiring less parameters than other approaches.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <a href="https://rl-conference.cc" rel="external nofollow noopener" target="_blank">RLC</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/rlc_2024.gif" sizes="200px"> <img src="/assets/img/publication_preview/rlc_2024.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="rlc_2024.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="vasco2024superhuman" class="col-sm-8"> <div class="title">A Super-human Vision-based Reinforcement Learning Agent for Autonomous Racing in Gran Turismo</div> <div class="author"> <em>Miguel Vasco<sup>*</sup></em>, Takuma Seno<sup>*</sup>, Kenta Kawamoto, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Kaushik Subramanian, Peter R. Wurman, Peter Stone' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="*Shared first-authorship"> </i> </div> <div class="periodical"> <em>Reinforcement Learning Journal</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Outstanding Paper Award</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ai.sony/publications/A-Super-human-Vision-based-Reinforcement-Learning-Agent-for-Autonomous-Racing-in-Gran-Turismo/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2406.12563" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Outstading Paper Award in Applications of RL</p> </div> <div class="abstract hidden"> <p>Racing autonomous cars faster than the best human drivers has been a longstanding grand challenge for the fields of Artificial Intelligence and robotics. Recently, an end-to-end deep reinforcement learning agent met this challenge in a high-fidelity racing simulator, Gran Turismo. However, this agent relied on global features that require instrumentation external to the car. This paper introduces, to the best of our knowledge, the first super-human car racing agent whose sensor input is purely local to the car, namely pixels from an ego-centric camera view and quantities that can be sensed from on-board the car, such as the car’s velocity. By leveraging global features only at training time, the learned agent is able to outperform the best human drivers in time trial (one car on the track at a time) races using only local input features. The resulting agent is evaluated in Gran Turismo 7 on multiple tracks and cars. Detailed ablation experiments demonstrate the agent’s strong reliance on visual inputs, making it the first vision-based super-human car racing agent.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#9f9f00"> <a href="https://icml.cc" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/gmc_2022.png" sizes="200px"> <img src="/assets/img/publication_preview/gmc_2022.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="gmc_2022.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="poklukar2022gmc" class="col-sm-8"> <div class="title">Geometric Multimodal Contrastive Representation Learning</div> <div class="author"> Petra Poklukar<sup>*</sup>, <em>Miguel Vasco<sup>*</sup></em>, Hang Yin, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Francisco S. Melo, Ana Paiva, Danica Kragic' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="*Shared first-authorship"> </i> </div> <div class="periodical"> <em>In Proceedings of the 39th International Conference on Machine Learning</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2202.03390" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2202.03390" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/miguelsvasco/gmc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Learning representations of multimodal data that are both informative and robust to missing modalities at test time remains a challenging problem due to the inherent heterogeneity of data obtained from different channels. To address it, we present a novel Geometric Multimodal Contrastive (GMC) representation learning method consisting of two main components: i) a two-level architecture consisting of modality-specific base encoders, allowing to process an arbitrary number of modalities to an intermediate representation of fixed dimensionality, and a shared projection head, mapping the intermediate representations to a latent representation space; ii) a multimodal contrastive loss function that encourages the geometric alignment of the learned representations. We experimentally demonstrate that GMC representations are semantically rich and achieve state-of-the-art performance with missing modality information on three different learning problems including prediction and reinforcement learning tasks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://aamas2025.org" rel="external nofollow noopener" target="_blank">AAMAS</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/aamas_2022.gif" sizes="200px"> <img src="/assets/img/publication_preview/aamas_2022.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="aamas_2022.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="vasco2022sense" class="col-sm-8"> <div class="title">How to Sense the World: Leveraging Hierarchy in Multimodal Perception for Robust Reinforcement Learning Agents</div> <div class="author"> <em>Miguel Vasco</em>, Hang Yin, Francisco S. Melo, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Ana Paiva' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 21st International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2110.03608" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ifaamas.org/Proceedings/aamas2022/pdfs/p1301.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/miguelsvasco/muse" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This work addresses the problem of sensing the world: how to learn a multimodal representation of a reinforcement learning agent’s environment that allows the execution of tasks under incomplete perceptual conditions. To address such problem, we argue for hierarchy in the design of representation models and contribute with a novel multimodal representation model, MUSE. The proposed model learns hierarchical representations: low-level modality-specific representations, encoded from raw observation data, and a high-level multimodal representation, encoding joint-modality information to allow robust state estimation. We employ MUSE as the sensory representation model of deep reinforcement learning agents provided with multimodal observations in Atari games. We perform a comparative study over different designs of reinforcement learning agents, showing that MUSE allows agents to perform tasks under incomplete perceptual experience with minimal performance loss. Finally, we evaluate the performance of MUSE in literature-standard multimodal scenarios with higher number and more complex modalities, showing that it outperforms state-of-the-art multimodal variational autoencoders in single and cross-modality generation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Neural Networks</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/nn_2022.png" sizes="200px"> <img src="/assets/img/publication_preview/nn_2022.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="nn_2022.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="vasco2022leveraging" class="col-sm-8"> <div class="title">Leveraging hierarchy in multimodal generative models for effective cross-modality inference</div> <div class="author"> <em>Miguel Vasco</em>, Hang Yin, Francisco S. Melo, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Ana Paiva' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Neural Networks (2021 Special Issue on AI and Brain Science: Brain-inspired AI)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S0893608021004470" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/miguelsvasco/nexus_pytorch" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This work addresses the problem of sensing the world: how to learn a multimodal representation of a reinforcement learning agent’s environment that allows the execution of tasks under incomplete perceptual conditions. To address such problem, we argue for hierarchy in the design of representation models and contribute with a novel multimodal representation model, MUSE. The proposed model learns hierarchical representations: low-level modality-specific representations, encoded from raw observation data, and a high-level multimodal representation, encoding joint-modality information to allow robust state estimation. We employ MUSE as the sensory representation model of deep reinforcement learning agents provided with multimodal observations in Atari games. We perform a comparative study over different designs of reinforcement learning agents, showing that MUSE allows agents to perform tasks under incomplete perceptual experience with minimal performance loss. Finally, we evaluate the performance of MUSE in literature-standard multimodal scenarios with higher number and more complex modalities, showing that it outperforms state-of-the-art multimodal variational autoencoders in single and cross-modality generation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://aamas2025.org" rel="external nofollow noopener" target="_blank">AAMAS</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/gamesdark_2020.png" sizes="200px"> <img src="/assets/img/publication_preview/gamesdark_2020.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="gamesdark_2020.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="silva2020playing" class="col-sm-8"> <div class="title">Playing Games in the Dark: An Approach for Cross-Modality Transfer in Reinforcement Learning</div> <div class="author"> Rui Silva, <em>Miguel Vasco</em>, Francisco S. Melo, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Ana Paiva, Manuela Veloso' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In 19th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/1911.12851" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://aamas.csc.liv.ac.uk/Proceedings/aamas2020/pdfs/p1260.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/miguelsvasco/multimodal-atari-games" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In this work we explore the use of latent representations obtained from multiple input sensory modalities (such as images or sounds) in allowing an agent to learn and exploit policies over different subsets of input modalities. We propose a three-stage architecture that allows a reinforcement learning agent trained over a given sensory modality, to execute its task on a different sensory modality-for example, learning a visual policy over image inputs, and then execute such policy when only sound inputs are available. We show that the generalized policies achieve better out-of-the-box performance when compared to different baselines. Moreover, we show this holds in different OpenAI gym and video game environments, even when using different multimodal generative models and reinforcement learning algorithms.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6D%69%67%75%65%6C%73%76@%6B%74%68.%73%65" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=Of2hDmMAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/miguelsvasco" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/miguelsvasco" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://bsky.app/profile/miguelvasco.bsky.social" title="Bluesky" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-bluesky"></i></a> </div> <div class="contact-note">I'm currently on the job market for industry positions so feel free to send me an email. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Miguel Vasco. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"here is a list of my publications. For the most recent, always check google scholar!",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/files/CV.pdf"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"news-our-paper-on-super-human-autonomous-racing-won-an-outstanding-paper-award-at-rlc-2024",title:"Our paper on super-human autonomous racing won an Outstanding Paper Award at RLC...",description:"",section:"News"},{id:"news-won-the-best-phd-thesis-in-ai-in-portugal-award-by-the-portuguese-association-for-artificial-intelligence-appia",title:"Won the Best PhD Thesis in AI in Portugal award by the Portuguese...",description:"",section:"News"},{id:"news-two-papers-accepted-at-neurips-2024-our-first-work-explores-foundation-models-of-chemical-data-for-human-olfaction-spotlight-link-our-second-work-explores-learning-decision-making-algorithms-that-scale-to-arbitrarily-large-observation-spaces-poster-link",title:"Two Papers Accepted at NeurIPS 2024! Our first work explores foundation models of...",description:"",section:"News"},{id:"news-our-work-on-reducing-variance-in-meta-learning-was-accepted-at-tmlr-link",title:"Our work on reducing variance in meta-learning was accepted at TMLR (link)!",description:"",section:"News"},{id:"news-our-work-on-sample-efficient-adaptation-of-reward-models-for-preference-based-rl-has-been-accepted-at-icra-link",title:"Our work on sample-efficient adaptation of reward models for preference-based RL has been...",description:"",section:"News"},{id:"news-our-paper-on-exploring-early-stopping-bias-in-the-low-data-regime-of-deep-learning-has-been-accepted-at-icassp-link",title:"Our paper on exploring early stopping bias in the low-data regime of deep...",description:"",section:"News"},{id:"news-new-preprints-on-the-alignment-of-image-models-for-visual-decoding-from-the-brain-link-and-on-a-framework-for-the-long-term-co-existence-between-humans-and-artificial-agents-link",title:"New preprints on the alignment of image models for visual decoding from the...",description:"",section:"News"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6D%69%67%75%65%6C%73%76@%6B%74%68.%73%65","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=Of2hDmMAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/miguelsvasco","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/miguelsvasco","_blank")}},{id:"socials-bluesky",title:"Bluesky",section:"Socials",handler:()=>{window.open("https://bsky.app/profile/miguelvasco.bsky.social","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>